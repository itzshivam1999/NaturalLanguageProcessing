{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681f53e5-0e18-4267-beba-2998a26e8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('kindle_review.csv')\n",
    "df = data[['reviewText', 'rating']].copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af624d3-42fb-410f-a891-96829a3b9887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jace Rankin may be short, but he's nothing to ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great short read.  I didn't want to put it dow...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll start by saying this is the first of four...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aggie is Angela Lansbury who carries pocketboo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I did not expect this type of book to be in li...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>Valentine cupid is a vampire- Jena and Ian ano...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>I have read all seven books in this series. Ap...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>This book really just wasn't my cuppa.  The si...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>tried to use it to charge my kindle, it didn't...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>Taking Instruction is a look into the often hi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              reviewText  rating\n",
       "0      Jace Rankin may be short, but he's nothing to ...       3\n",
       "1      Great short read.  I didn't want to put it dow...       5\n",
       "2      I'll start by saying this is the first of four...       3\n",
       "3      Aggie is Angela Lansbury who carries pocketboo...       3\n",
       "4      I did not expect this type of book to be in li...       4\n",
       "...                                                  ...     ...\n",
       "11995  Valentine cupid is a vampire- Jena and Ian ano...       4\n",
       "11996  I have read all seven books in this series. Ap...       5\n",
       "11997  This book really just wasn't my cuppa.  The si...       3\n",
       "11998  tried to use it to charge my kindle, it didn't...       1\n",
       "11999  Taking Instruction is a look into the often hi...       3\n",
       "\n",
       "[12000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f583f9b4-a3f8-48d7-a423-eefc8316a25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\itzsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\itzsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\itzsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\itzsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\itzsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\itzsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Convert rating to binary (0: negative, 1: positive)\n",
    "df['rating'] = df['rating'].apply(lambda x: 0 if x < 3 else 1)\n",
    "\n",
    "# Lowercase\n",
    "df['reviewText'] = df['reviewText'].str.lower()\n",
    "\n",
    "# Remove special characters, URLs, and HTML tags\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', str(x)))\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words'])\n",
    "\n",
    "# Tokenization\n",
    "df['tokens'] = df['reviewText'].apply(word_tokenize)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmed'] = df['tokens'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n",
    "\n",
    "# POS Tagging\n",
    "df['pos_tags'] = df['tokens'].apply(pos_tag)\n",
    "\n",
    "# Lemmatization with POS\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_with_pos(tagged_tokens):\n",
    "    lemmatized = []\n",
    "    for word, tag in tagged_tokens:\n",
    "        pos = tag[0].lower()\n",
    "        pos = pos if pos in ['a', 'r', 'n', 'v'] else 'n'  # Default to noun\n",
    "        lemmatized.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized\n",
    "\n",
    "df['lemmatized'] = df['pos_tags'].apply(lemmatize_with_pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d660664-5244-4190-bd16-0430b7261264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Bag of Words (BOW)\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_bow = bow_vectorizer.fit_transform(df['lemmatized'].apply(' '.join))\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['lemmatized'].apply(' '.join))\n",
    "\n",
    "# Word2Vec\n",
    "# Train Word2Vec model\n",
    "sentences = df['lemmatized'].tolist()\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert sentences to vectors by averaging word vectors\n",
    "def sentence_vector(sentence):\n",
    "    return np.mean([w2v_model.wv[word] for word in sentence if word in w2v_model.wv], axis=0)\n",
    "\n",
    "X_w2v = np.array([sentence_vector(sentence) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7383a45-62e8-4b6c-8f1d-086bf85bdc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>rating</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jace rankin may be short but hes nothing to me...</td>\n",
       "      <td>1</td>\n",
       "      <td>[jace, rankin, may, short, hes, nothing, mess,...</td>\n",
       "      <td>[jace, rankin, may, short, he, noth, mess, man...</td>\n",
       "      <td>[(jace, NN), (rankin, NN), (may, MD), (short, ...</td>\n",
       "      <td>[jace, rankin, may, short, he, nothing, mess, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great short read  i didnt want to put it down ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, short, read, didnt, want, put, read, o...</td>\n",
       "      <td>[great, short, read, didnt, want, put, read, o...</td>\n",
       "      <td>[(great, JJ), (short, JJ), (read, NN), (didnt,...</td>\n",
       "      <td>[great, short, read, didnt, want, put, read, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ill start by saying this is the first of four ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ill, start, saying, first, four, books, wasnt...</td>\n",
       "      <td>[ill, start, say, first, four, book, wasnt, ex...</td>\n",
       "      <td>[(ill, JJ), (start, VB), (saying, VBG), (first...</td>\n",
       "      <td>[ill, start, say, first, four, book, wasnt, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aggie is angela lansbury who carries pocketboo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[aggie, angela, lansbury, carries, pocketbooks...</td>\n",
       "      <td>[aggi, angela, lansburi, carri, pocketbook, in...</td>\n",
       "      <td>[(aggie, NN), (angela, JJ), (lansbury, NN), (c...</td>\n",
       "      <td>[aggie, angela, lansbury, carry, pocketbook, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i did not expect this type of book to be in li...</td>\n",
       "      <td>1</td>\n",
       "      <td>[expect, type, book, library, pleased, find, p...</td>\n",
       "      <td>[expect, type, book, librari, pleas, find, pri...</td>\n",
       "      <td>[(expect, VB), (type, NN), (book, NN), (librar...</td>\n",
       "      <td>[expect, type, book, library, please, find, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>valentine cupid is a vampire jena and ian anot...</td>\n",
       "      <td>1</td>\n",
       "      <td>[valentine, cupid, vampire, jena, ian, another...</td>\n",
       "      <td>[valentin, cupid, vampir, jena, ian, anoth, va...</td>\n",
       "      <td>[(valentine, NN), (cupid, NN), (vampire, NN), ...</td>\n",
       "      <td>[valentine, cupid, vampire, jena, ian, another...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>i have read all seven books in this series apo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[read, seven, books, series, apocalypticadvent...</td>\n",
       "      <td>[read, seven, book, seri, apocalypticadventur,...</td>\n",
       "      <td>[(read, VB), (seven, CD), (books, NNS), (serie...</td>\n",
       "      <td>[read, seven, book, series, apocalypticadventu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>this book really just wasnt my cuppa  the situ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[book, really, wasnt, cuppa, situation, man, c...</td>\n",
       "      <td>[book, realli, wasnt, cuppa, situat, man, capt...</td>\n",
       "      <td>[(book, NN), (really, RB), (wasnt, JJ), (cuppa...</td>\n",
       "      <td>[book, really, wasnt, cuppa, situation, man, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>tried to use it to charge my kindle it didnt e...</td>\n",
       "      <td>0</td>\n",
       "      <td>[tried, use, charge, kindle, didnt, even, regi...</td>\n",
       "      <td>[tri, use, charg, kindl, didnt, even, regist, ...</td>\n",
       "      <td>[(tried, VBN), (use, JJ), (charge, NN), (kindl...</td>\n",
       "      <td>[try, use, charge, kindle, didnt, even, regist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>taking instruction is a look into the often hi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[taking, instruction, look, often, hidden, wor...</td>\n",
       "      <td>[take, instruct, look, often, hidden, world, s...</td>\n",
       "      <td>[(taking, VBG), (instruction, NN), (look, NN),...</td>\n",
       "      <td>[take, instruction, look, often, hidden, world...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              reviewText  rating  \\\n",
       "0      jace rankin may be short but hes nothing to me...       1   \n",
       "1      great short read  i didnt want to put it down ...       1   \n",
       "2      ill start by saying this is the first of four ...       1   \n",
       "3      aggie is angela lansbury who carries pocketboo...       1   \n",
       "4      i did not expect this type of book to be in li...       1   \n",
       "...                                                  ...     ...   \n",
       "11995  valentine cupid is a vampire jena and ian anot...       1   \n",
       "11996  i have read all seven books in this series apo...       1   \n",
       "11997  this book really just wasnt my cuppa  the situ...       1   \n",
       "11998  tried to use it to charge my kindle it didnt e...       0   \n",
       "11999  taking instruction is a look into the often hi...       1   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [jace, rankin, may, short, hes, nothing, mess,...   \n",
       "1      [great, short, read, didnt, want, put, read, o...   \n",
       "2      [ill, start, saying, first, four, books, wasnt...   \n",
       "3      [aggie, angela, lansbury, carries, pocketbooks...   \n",
       "4      [expect, type, book, library, pleased, find, p...   \n",
       "...                                                  ...   \n",
       "11995  [valentine, cupid, vampire, jena, ian, another...   \n",
       "11996  [read, seven, books, series, apocalypticadvent...   \n",
       "11997  [book, really, wasnt, cuppa, situation, man, c...   \n",
       "11998  [tried, use, charge, kindle, didnt, even, regi...   \n",
       "11999  [taking, instruction, look, often, hidden, wor...   \n",
       "\n",
       "                                                 stemmed  \\\n",
       "0      [jace, rankin, may, short, he, noth, mess, man...   \n",
       "1      [great, short, read, didnt, want, put, read, o...   \n",
       "2      [ill, start, say, first, four, book, wasnt, ex...   \n",
       "3      [aggi, angela, lansburi, carri, pocketbook, in...   \n",
       "4      [expect, type, book, librari, pleas, find, pri...   \n",
       "...                                                  ...   \n",
       "11995  [valentin, cupid, vampir, jena, ian, anoth, va...   \n",
       "11996  [read, seven, book, seri, apocalypticadventur,...   \n",
       "11997  [book, realli, wasnt, cuppa, situat, man, capt...   \n",
       "11998  [tri, use, charg, kindl, didnt, even, regist, ...   \n",
       "11999  [take, instruct, look, often, hidden, world, s...   \n",
       "\n",
       "                                                pos_tags  \\\n",
       "0      [(jace, NN), (rankin, NN), (may, MD), (short, ...   \n",
       "1      [(great, JJ), (short, JJ), (read, NN), (didnt,...   \n",
       "2      [(ill, JJ), (start, VB), (saying, VBG), (first...   \n",
       "3      [(aggie, NN), (angela, JJ), (lansbury, NN), (c...   \n",
       "4      [(expect, VB), (type, NN), (book, NN), (librar...   \n",
       "...                                                  ...   \n",
       "11995  [(valentine, NN), (cupid, NN), (vampire, NN), ...   \n",
       "11996  [(read, VB), (seven, CD), (books, NNS), (serie...   \n",
       "11997  [(book, NN), (really, RB), (wasnt, JJ), (cuppa...   \n",
       "11998  [(tried, VBN), (use, JJ), (charge, NN), (kindl...   \n",
       "11999  [(taking, VBG), (instruction, NN), (look, NN),...   \n",
       "\n",
       "                                              lemmatized  \n",
       "0      [jace, rankin, may, short, he, nothing, mess, ...  \n",
       "1      [great, short, read, didnt, want, put, read, o...  \n",
       "2      [ill, start, say, first, four, book, wasnt, ex...  \n",
       "3      [aggie, angela, lansbury, carry, pocketbook, i...  \n",
       "4      [expect, type, book, library, please, find, pr...  \n",
       "...                                                  ...  \n",
       "11995  [valentine, cupid, vampire, jena, ian, another...  \n",
       "11996  [read, seven, book, series, apocalypticadventu...  \n",
       "11997  [book, really, wasnt, cuppa, situation, man, c...  \n",
       "11998  [try, use, charge, kindle, didnt, even, regist...  \n",
       "11999  [take, instruction, look, often, hidden, world...  \n",
       "\n",
       "[12000 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71480fea-f47c-40b1-9ab0-ec644d7e6ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Accuracy: 0.58625\n",
      "TF-IDF Accuracy: 0.4483333333333333\n",
      "Word2Vec Accuracy: 0.55875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split data\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, df['rating'], test_size=0.2)\n",
    "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(X_tfidf, df['rating'], test_size=0.2)\n",
    "X_train_w2v, X_test_w2v, _, _ = train_test_split(X_w2v, df['rating'], test_size=0.2)\n",
    "\n",
    "# Train models\n",
    "bow = GaussianNB().fit(X_train_bow.toarray(), y_train)\n",
    "tfidf = GaussianNB().fit(X_train_tfidf.toarray(), y_train)\n",
    "w2v = GaussianNB().fit(X_train_w2v, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_bow = bow.predict(X_test_bow.toarray())\n",
    "y_pred_tfidf = tfidf.predict(X_test_tfidf.toarray())\n",
    "y_pred_w2v = w2v.predict(X_test_w2v)\n",
    "\n",
    "print(\"BOW Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
    "print(\"TF-IDF Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
    "print(\"Word2Vec Accuracy:\", accuracy_score(y_test, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe0254a-eb8f-4d3a-83d6-ed0e3381871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Accuracy: 0.795\n",
      "TF-IDF Accuracy: 0.6695833333333333\n",
      "Word2Vec Accuracy: 0.665\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "bow = RandomForestClassifier().fit(X_train_bow.toarray(), y_train)\n",
    "tfidf = RandomForestClassifier().fit(X_train_tfidf.toarray(), y_train)\n",
    "w2v = RandomForestClassifier().fit(X_train_w2v, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_bow = bow.predict(X_test_bow.toarray())\n",
    "y_pred_tfidf = tfidf.predict(X_test_tfidf.toarray())\n",
    "y_pred_w2v = w2v.predict(X_test_w2v)\n",
    "\n",
    "print(\"BOW Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
    "print(\"TF-IDF Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
    "print(\"Word2Vec Accuracy:\", accuracy_score(y_test, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4377fcde-ba2e-46c5-b5e5-6dc488768a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Accuracy: 0.82\n",
      "TF-IDF Accuracy: 0.6670833333333334\n",
      "Word2Vec Accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "bow = SVC(kernel='linear').fit(X_train_bow.toarray(), y_train)\n",
    "tfidf = SVC(kernel='linear').fit(X_train_tfidf.toarray(), y_train)\n",
    "w2v = SVC(kernel='linear').fit(X_train_w2v, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_bow = bow.predict(X_test_bow.toarray())\n",
    "y_pred_tfidf = tfidf.predict(X_test_tfidf.toarray())\n",
    "y_pred_w2v = w2v.predict(X_test_w2v)\n",
    "\n",
    "print(\"BOW Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
    "print(\"TF-IDF Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
    "print(\"Word2Vec Accuracy:\", accuracy_score(y_test, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dafe4bac-e69c-41e5-9be8-486545ba2531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.70      0.75       803\n",
      "           1       0.86      0.92      0.89      1597\n",
      "\n",
      "    accuracy                           0.85      2400\n",
      "   macro avg       0.84      0.81      0.82      2400\n",
      "weighted avg       0.85      0.85      0.84      2400\n",
      "\n",
      "0.8470833333333333\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Features (reuse existing)\n",
    "X = tfidf_vectorizer.fit_transform(df['lemmatized'].apply(' '.join))\n",
    "y = df['rating']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# SVM Model\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "print(\"SVM Report:\\n\", classification_report(y_test, y_pred_svm))\n",
    "print(accuracy_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ce32f8-c274-4ccc-9cad-b1fb3a99e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.45      0.60       803\n",
      "           1       0.78      0.96      0.86      1597\n",
      "\n",
      "    accuracy                           0.79      2400\n",
      "   macro avg       0.82      0.71      0.73      2400\n",
      "weighted avg       0.81      0.79      0.77      2400\n",
      "\n",
      "0.7933333333333333\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "049088b8-0a62-47f7-8e2f-b582cb00b2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.73       803\n",
      "           1       0.86      0.87      0.86      1597\n",
      "\n",
      "    accuracy                           0.82      2400\n",
      "   macro avg       0.80      0.79      0.79      2400\n",
      "weighted avg       0.82      0.82      0.82      2400\n",
      "\n",
      "0.81875\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Features (reuse existing)\n",
    "X = bow_vectorizer.fit_transform(df['lemmatized'].apply(' '.join))\n",
    "y = df['rating']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# SVM Model\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "print(\"SVM Report:\\n\", classification_report(y_test, y_pred_svm))\n",
    "print(accuracy_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcc45fe2-a5dc-4c89-84e9-b427bce1de5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.46      0.59       803\n",
      "           1       0.78      0.96      0.86      1597\n",
      "\n",
      "    accuracy                           0.79      2400\n",
      "   macro avg       0.81      0.71      0.73      2400\n",
      "weighted avg       0.80      0.79      0.77      2400\n",
      "\n",
      "0.79125\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "912c6086-9c5d-4f57-8f36-26a0d5c988b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itzsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 71ms/step - accuracy: 0.6899 - loss: 0.5924 - val_accuracy: 0.7575 - val_loss: 0.4763\n",
      "Epoch 2/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 68ms/step - accuracy: 0.7940 - loss: 0.4433 - val_accuracy: 0.7254 - val_loss: 0.5694\n",
      "Epoch 3/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.7972 - loss: 0.4316 - val_accuracy: 0.7879 - val_loss: 0.4498\n",
      "Epoch 4/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.8234 - loss: 0.3829 - val_accuracy: 0.7821 - val_loss: 0.4435\n",
      "Epoch 5/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 68ms/step - accuracy: 0.8467 - loss: 0.3513 - val_accuracy: 0.7812 - val_loss: 0.4414\n",
      "Epoch 6/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.8551 - loss: 0.3313 - val_accuracy: 0.7892 - val_loss: 0.4376\n",
      "Epoch 7/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.8788 - loss: 0.2933 - val_accuracy: 0.7987 - val_loss: 0.4308\n",
      "Epoch 8/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.8835 - loss: 0.2862 - val_accuracy: 0.7817 - val_loss: 0.4555\n",
      "Epoch 9/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 69ms/step - accuracy: 0.8995 - loss: 0.2552 - val_accuracy: 0.7550 - val_loss: 0.6182\n",
      "Epoch 10/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 69ms/step - accuracy: 0.9100 - loss: 0.2302 - val_accuracy: 0.7829 - val_loss: 0.4664\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7766 - loss: 0.4621\n",
      "CNN Accuracy: 0.7829\n"
     ]
    }
   ],
   "source": [
    "# CNN Model using existing Word2Vec embeddings\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert lemmatized tokens to sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['lemmatized'].apply(lambda x: ' '.join(x)))\n",
    "sequences = tokenizer.texts_to_sequences(df['lemmatized'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max(len(s) for s in sequences)\n",
    "X_cnn = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "y = df['rating']\n",
    "\n",
    "# Split data with fixed random_state\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load existing Word2Vec embeddings\n",
    "embedding_dim = 100\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# Build CNN model\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model_cnn.add(Conv1D(128, 5, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train and evaluate\n",
    "model_cnn.fit(X_train_cnn, y_train_cnn, epochs=10, validation_data=(X_test_cnn, y_test_cnn))\n",
    "loss, accuracy = model_cnn.evaluate(X_test_cnn, y_test_cnn)\n",
    "print(f\"CNN Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a640a2-444d-4af7-b636-c5697b23d576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itzsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 497ms/step - accuracy: 0.6526 - loss: 1.4850 - val_accuracy: 0.7096 - val_loss: 0.7683\n",
      "Epoch 2/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 532ms/step - accuracy: 0.7241 - loss: 0.6896 - val_accuracy: 0.6792 - val_loss: 0.6379\n",
      "Epoch 3/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 548ms/step - accuracy: 0.7747 - loss: 0.5280 - val_accuracy: 0.8179 - val_loss: 0.4324\n",
      "Epoch 4/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 543ms/step - accuracy: 0.8103 - loss: 0.4605 - val_accuracy: 0.8158 - val_loss: 0.4199\n",
      "Epoch 5/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 539ms/step - accuracy: 0.8221 - loss: 0.4156 - val_accuracy: 0.8071 - val_loss: 0.4201\n",
      "Epoch 6/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 546ms/step - accuracy: 0.8444 - loss: 0.3751 - val_accuracy: 0.8117 - val_loss: 0.4123\n",
      "Epoch 7/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 545ms/step - accuracy: 0.8604 - loss: 0.3383 - val_accuracy: 0.8575 - val_loss: 0.3531\n",
      "Epoch 8/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 554ms/step - accuracy: 0.8867 - loss: 0.2950 - val_accuracy: 0.8504 - val_loss: 0.3588\n",
      "Epoch 9/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 544ms/step - accuracy: 0.9091 - loss: 0.2481 - val_accuracy: 0.8367 - val_loss: 0.3840\n",
      "Epoch 10/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 545ms/step - accuracy: 0.9290 - loss: 0.2047 - val_accuracy: 0.8400 - val_loss: 0.4293\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.8322 - loss: 0.4397\n",
      "Improved CNN + LSTM Accuracy with Word2Vec: 0.8400\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization, Bidirectional, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('kindle_review.csv')\n",
    "df['rating'] = df['rating'].apply(lambda x: 0 if x < 3 else 1)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['reviewText'])\n",
    "sequences = tokenizer.texts_to_sequences(df['reviewText'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 200\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "y = df['rating']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Word2Vec model\n",
    "sentences = [text.split() for text in df['reviewText']]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# Learning rate scheduling\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.9, staircase=True)\n",
    "\n",
    "# Build CNN + BiLSTM model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True),\n",
    "    SpatialDropout1D(0.3),\n",
    "\n",
    "    # CNN Layers\n",
    "    Conv1D(128, 5, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv1D(64, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Bidirectional LSTM for contextual understanding\n",
    "    Bidirectional(LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    GlobalMaxPooling1D(),\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Improved CNN + LSTM Accuracy with Word2Vec: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "377faa08-340f-4de6-bfe3-e49bcd9ee295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itzsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 798ms/step - accuracy: 0.6586 - loss: 1.5121 - val_accuracy: 0.7000 - val_loss: 0.8156\n",
      "Epoch 2/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 773ms/step - accuracy: 0.7096 - loss: 0.7357 - val_accuracy: 0.8079 - val_loss: 0.4983\n",
      "Epoch 3/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 783ms/step - accuracy: 0.7998 - loss: 0.5011 - val_accuracy: 0.8367 - val_loss: 0.4146\n",
      "Epoch 4/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 779ms/step - accuracy: 0.8392 - loss: 0.4055 - val_accuracy: 0.8546 - val_loss: 0.3649\n",
      "Epoch 5/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 782ms/step - accuracy: 0.8694 - loss: 0.3449 - val_accuracy: 0.8571 - val_loss: 0.3497\n",
      "Epoch 6/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 780ms/step - accuracy: 0.8749 - loss: 0.3214 - val_accuracy: 0.8633 - val_loss: 0.3482\n",
      "Epoch 7/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 788ms/step - accuracy: 0.8952 - loss: 0.2715 - val_accuracy: 0.8492 - val_loss: 0.3807\n",
      "Epoch 8/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 780ms/step - accuracy: 0.9101 - loss: 0.2489 - val_accuracy: 0.8600 - val_loss: 0.3451\n",
      "Epoch 9/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 783ms/step - accuracy: 0.9175 - loss: 0.2302 - val_accuracy: 0.8571 - val_loss: 0.3671\n",
      "Epoch 10/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 778ms/step - accuracy: 0.9325 - loss: 0.1963 - val_accuracy: 0.8508 - val_loss: 0.3773\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.8423 - loss: 0.4002\n",
      "Improved CNN + LSTM Accuracy with Glo: 0.8508\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization, Bidirectional, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('kindle_review.csv')\n",
    "df['rating'] = df['rating'].apply(lambda x: 0 if x < 3 else 1)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['reviewText'])\n",
    "sequences = tokenizer.texts_to_sequences(df['reviewText'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 200\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "y = df['rating']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained Glove embeddings\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in glove_model:\n",
    "        embedding_matrix[i] = glove_model[word]\n",
    "\n",
    "# Learning rate scheduling\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.99, staircase=True)\n",
    "\n",
    "# Build improved CNN + LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True),\n",
    "    SpatialDropout1D(0.3),\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    Conv1D(128, 5, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    Conv1D(64, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # LSTM Layer with Residual Connection\n",
    "    Bidirectional(LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Improved CNN + LSTM Accuracy with GloVe: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569926aa-e217-4086-9a38-f2890b1f9569",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
